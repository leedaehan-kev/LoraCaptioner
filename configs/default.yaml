batch_size: 64
device: cuda
img_encoder_name: ViT-B/16
log_interval_batch: 50
lora_config:
  lora_alpha: 16
  lora_dropout: 0.1
  r: 8
  target_modules:
  - SelfAttention.q
  - SelfAttention.k
  - EncDecAttention.q
  - EncDecAttention.k
  - DenseReluDense.wi_0
  - DenseReluDense.wi_1
  - DenseReluDense.wo
lr: 0.0001
num_epochs: 8
num_workers: 2
text_encdec_name: google/flan-t5-base
data_subset_size: 300000
